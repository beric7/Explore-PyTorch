{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch_Regression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gyQnQ9gTCdUE","colab_type":"text"},"source":["Regression from Scratch in Numpy vs. PyTorch"]},{"cell_type":"markdown","metadata":{"id":"FMCnNSexCly1","colab_type":"text"},"source":["Regression in Numpy"]},{"cell_type":"code","metadata":{"id":"CQ_JDYDgOg0u","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cBkp4CiCskO","colab_type":"code","colab":{}},"source":["X = np.array([1,2,4,6,8,10,12,13,14,16,16,18,20,22,24])\n","Y = np.array([39,42,43,46,47,56,60,59,64,66,68,72,71,75,80])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kD1fuz_bCsZ0","colab_type":"code","outputId":"998e1724-98f3-4f66-a0ca-7074e060ff9f","executionInfo":{"status":"ok","timestamp":1577957652974,"user_tz":-330,"elapsed":1622,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["plt.scatter(X,Y)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7fc3f3b97c18>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuiQVxFCXpIL8H7wM9jc4Qe2VAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"OIn7YAkCOzQv","colab_type":"text"},"source":["As we can see there is linear relationship between X and Y.(We'll discuss more about correlation in another post). We'll use linear regression to build prediction model"]},{"cell_type":"code","metadata":{"id":"JAUgXRvdCsR4","colab_type":"code","outputId":"d4595310-3e73-4306-e458-11a1f1e98e9b","executionInfo":{"status":"ok","timestamp":1577957652976,"user_tz":-330,"elapsed":1597,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''Y = a*X+b is the equation of line/linear regression model.\n","Goal here is to find the values of a and b.\n","There are multiple techniques to achieve this:\n","1.Matrix calculations: Put all data into matrices to perform optimization.Used for small dataset because of memory requirement.\n","2.Gradient Descent : Try to minimize error/difference between actual and predicted values using derivatives.\n","3.Regularization: While minimizing error,also try to reduce impact of unnecessary features.\n","4.Simple linear regression:If there are single input varaible and single output variable,use covariance and variance to find a and b.\n","\n","More detailed explaination of above techniques is not in the scope here.\n","We'll implement method 2 i.e Gradient Descent here-more specific-Batch Gradient Descent.\n","Weights(a,b) are updated at end of complete batch/all rows as follow:\n","new a = old a - (learning_rate*gradient_a)\n","new b = old b - (learning_rate*gradient_b)\n","\n","'''\n"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Y = a*X+b is the equation of line/linear regression model.\\nGoal here is to find the values of a and b.\\nThere are multiple techniques to achieve this:\\n1.Matrix calculations: Put all data into matrices to perform optimization.Used for small dataset because of memory requirement.\\n2.Gradient Descent : Try to minimize error/difference between actual and predicted values using derivatives.\\n3.Regularization: While minimizing error,also try to reduce impact of unnecessary features.\\n4.Simple linear regression:If there are single input varaible and single output variable,use covariance and variance to find a and b.\\n\\nMore detailed explaination of above techniques is not in the scope here.\\nWe'll implement method 2 i.e Gradient Descent here-more specific-Batch Gradient Descent.\\nWeights(a,b) are updated at end of complete batch/all rows as follow:\\nnew a = old a - (learning_rate*gradient_a)\\nnew b = old b - (learning_rate*gradient_b)\\n\\n\""]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"ECaYuxM4zsIJ","colab_type":"code","colab":{}},"source":["np.random.seed(2)\n","epochs=15\n","learning_rate = 0.001\n","w = np.random.randn()\n","b = np.random.randn()\n","y_pred = np.empty(len(Y))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3449vzB3zsEG","colab_type":"code","outputId":"8eb28c8d-f0c0-43ec-c06d-8e890756a03e","executionInfo":{"status":"ok","timestamp":1577957675063,"user_tz":-330,"elapsed":1240,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["for i in range(epochs):\n","    print(\"-----------epoch:{}--------\".format(i))\n","    #prediction\n","    y_pred = w*X +b \n","\n","    #Error/loss calculation is Mean Squared Error\n","    error = np.mean((Y - y_pred)**2)  \n","    print('Total Error:{}'.format(error))\n","\n","    #Gradient calculation\n","    gradient_a = np.mean(-2*X*(Y-y_pred))\n","    gradient_b = np.mean(-2*(Y-y_pred))\n","\n","    #Update weights\n","    w -= learning_rate*gradient_a\n","    b -= learning_rate*gradient_b\n","\n","  \n","   "],"execution_count":6,"outputs":[{"output_type":"stream","text":["-----------epoch:0--------\n","Total Error:4393.679411741339\n","-----------epoch:1--------\n","Total Error:1761.2933993839647\n","-----------epoch:2--------\n","Total Error:829.147694180196\n","-----------epoch:3--------\n","Total Error:498.94421785640026\n","-----------epoch:4--------\n","Total Error:381.8486450156428\n","-----------epoch:5--------\n","Total Error:340.20059390876725\n","-----------epoch:6--------\n","Total Error:325.26367603059253\n","-----------epoch:7--------\n","Total Error:319.7835764526846\n","-----------epoch:8--------\n","Total Error:317.6516893015521\n","-----------epoch:9--------\n","Total Error:316.705363217721\n","-----------epoch:10--------\n","Total Error:316.1789469743335\n","-----------epoch:11--------\n","Total Error:315.80137420708223\n","-----------epoch:12--------\n","Total Error:315.47667816740153\n","-----------epoch:13--------\n","Total Error:315.17088323869484\n","-----------epoch:14--------\n","Total Error:314.8719607426561\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hW2fVpbLbuyu","colab_type":"code","outputId":"f5d8af52-e59f-41cf-ddf2-e726e8006dd0","executionInfo":{"status":"ok","timestamp":1577957676642,"user_tz":-330,"elapsed":919,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(w,b)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["4.042799282999869 0.4771951521774575\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u-Oas24OAqQk","colab_type":"code","outputId":"d1252483-4996-418e-857f-67f656b11074","executionInfo":{"status":"ok","timestamp":1577957679936,"user_tz":-330,"elapsed":1473,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''Error is reducing with increment in epochs. Number of epochs and learning rate are hyperparameters to tune. \n","Let's not play around with it and jumpt to PyTorch'''"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Error is reducing with increment in epochs. Number of epochs and learning rate are hyperparameters to tune. \\nLet's not play around with it and jumpt to PyTorch\""]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"pmoHHxvV_X9N","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oht1yH1MCn9b","colab_type":"text"},"source":["Regression in PyTorch"]},{"cell_type":"code","metadata":{"id":"lPONm4D4Ft0J","colab_type":"code","colab":{}},"source":["import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iT7zh_6_AKq2","colab_type":"code","colab":{}},"source":["#initialise data/features and target\n","X_tensor = torch.from_numpy(X)\n","Y_tensor = torch.from_numpy(Y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTEuzbtFA5Ei","colab_type":"code","outputId":"c06372c8-c4bd-4fd5-b8b7-f2a66459f592","executionInfo":{"status":"ok","timestamp":1577957691615,"user_tz":-330,"elapsed":1271,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Initialise weights\n","'''Here unlike numpy we have to mention that these variables are trainable(need to calculate derivatives).\n","This can be done using requires_grad:'''"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Here unlike numpy we have to mention that these variables are trainable(need to calculate derivatives).\\nThis can be done using requires_grad:'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"tP9l7K-YICz2","colab_type":"code","colab":{}},"source":["torch.random.seed = 2\n","w_tensor = torch.randn(1,requires_grad=True,dtype=torch.float)\n","b_tensor = torch.randn(1,requires_grad=True,dtype=torch.float)\n","epochs=15\n","learning_rate = 0.001"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQHzW9E8cVSA","colab_type":"code","outputId":"1e9e69da-0fb0-4af1-cbc3-08397cda7ed0","executionInfo":{"status":"ok","timestamp":1577957695351,"user_tz":-330,"elapsed":1187,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["w_tensor"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.6845], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"0PBFdPCdUz9a","colab_type":"code","outputId":"33566e51-186e-4c6a-90b5-589610f0cc45","executionInfo":{"status":"ok","timestamp":1577957710818,"user_tz":-330,"elapsed":1496,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["#Model without PyTorch in-built methods\n","for i in range(epochs):\n","    print(\"-----------epoch:{}--------\".format(i))\n","    #prediction\n","    y_pred = w_tensor*X_tensor +b_tensor \n","\n","    #Error/loss calculation is Mean Squared Error\n","    error = ((Y_tensor - y_pred)**2).mean()\n","    print('Total Error:{}'.format(error))\n","\n","    '''Now no need to calculate gradients,PyTorch will do it if we tell which function/variable needs gradient calculation using backward()'''\n","    error.backward()\n","\n","    '''Actual values of gradients can be seen using grad attribute'''\n","    #print(w_tensor.grad,b_tensor.grad)\n","    \n","    '''We can not directly use gradients in normal calculation,so use no_grad() method to get variables out of scope of computation graph '''\n","    \n","    with torch.no_grad():\n","       w_tensor-= learning_rate*w_tensor.grad\n","       b_tensor-= learning_rate*b_tensor.grad\n","      \n","\n","     #After each step,Reinitilaise gradients because PyTorch holds on to gradients\n","    w_tensor.grad.zero_()\n","    b_tensor.grad.zero_()\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["-----------epoch:0--------\n","Total Error:4754.9716796875\n","-----------epoch:1--------\n","Total Error:1877.7296142578125\n","-----------epoch:2--------\n","Total Error:858.9075317382812\n","-----------epoch:3--------\n","Total Error:498.0286560058594\n","-----------epoch:4--------\n","Total Error:370.0838623046875\n","-----------epoch:5--------\n","Total Error:324.6056823730469\n","-----------epoch:6--------\n","Total Error:308.32354736328125\n","-----------epoch:7--------\n","Total Error:302.3780822753906\n","-----------epoch:8--------\n","Total Error:300.0922546386719\n","-----------epoch:9--------\n","Total Error:299.1021728515625\n","-----------epoch:10--------\n","Total Error:298.5710754394531\n","-----------epoch:11--------\n","Total Error:298.20263671875\n","-----------epoch:12--------\n","Total Error:297.8919372558594\n","-----------epoch:13--------\n","Total Error:297.6018371582031\n","-----------epoch:14--------\n","Total Error:297.3192443847656\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l8o0xyqjI6C_","colab_type":"code","outputId":"d1e6bc6a-7a63-4a39-f97c-8e59c0994bb8","executionInfo":{"status":"ok","timestamp":1577957712690,"user_tz":-330,"elapsed":1193,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["#Model with PyTorch in-built methods\n","optimizer = torch.optim.SGD([w_tensor, b_tensor], lr=learning_rate) \n","loss = torch.nn.MSELoss(reduction='mean')\n","for i in range(epochs):\n","    print(\"-----------epoch:{}--------\".format(i))\n","    #prediction\n","    y_pred = w_tensor*X_tensor +b_tensor \n","\n","    #Error/loss calculation is Mean Squared Error\n","    error = loss(Y_tensor, y_pred)  \n","    print('Total Error:{}'.format(error))\n","\n","    '''Now no need to calculate gradients,PyTorch will do it if we tell which function/variable needs gradient calculation using backward()'''\n","    error.backward()\n","\n","    #Update weights using Optimizer\n","    optimizer.step()\n","\n","    #After each step,Reinitilaise gradients because PyTorch holds on to gradients\n","    #Reinitilaise gradients using Optimizer\n","    optimizer.zero_grad()   "],"execution_count":15,"outputs":[{"output_type":"stream","text":["-----------epoch:0--------\n","Total Error:297.0393981933594\n","-----------epoch:1--------\n","Total Error:296.7607727050781\n","-----------epoch:2--------\n","Total Error:296.48272705078125\n","-----------epoch:3--------\n","Total Error:296.2050476074219\n","-----------epoch:4--------\n","Total Error:295.927734375\n","-----------epoch:5--------\n","Total Error:295.6506652832031\n","-----------epoch:6--------\n","Total Error:295.3738098144531\n","-----------epoch:7--------\n","Total Error:295.0972595214844\n","-----------epoch:8--------\n","Total Error:294.8209533691406\n","-----------epoch:9--------\n","Total Error:294.5448913574219\n","-----------epoch:10--------\n","Total Error:294.2691345214844\n","-----------epoch:11--------\n","Total Error:293.99365234375\n","-----------epoch:12--------\n","Total Error:293.7183532714844\n","-----------epoch:13--------\n","Total Error:293.4433898925781\n","-----------epoch:14--------\n","Total Error:293.16864013671875\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VcsTXlm9ZhHU","colab_type":"code","outputId":"e9cee805-510e-4145-8f98-216554ee3f94","executionInfo":{"status":"ok","timestamp":1577455933991,"user_tz":-330,"elapsed":674,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''Till now,we've explored loss calculation and Optimizers.\n","The only manual step remaining is prediction step. Let's remove that also'''\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Till now,we've explored loss calculation and Optimizers.\\nThe only manual step remaining is prediction step which also can be done by Model class,but we'll explore more on Model class and neural network in PyTorch in next \\narticle. If you liked the post CLAP!CLAP!CLAP! and your response is most WELCOME! \""]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"YI4z_msts1aG","colab_type":"code","colab":{}},"source":["#Create Network by extending parent nn.Module.\n","'''We have to implement __init__ and forward methods '''\n","class Network(torch.nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    #Intialise parameters whcih should be trained. Note that parameters need to be wrapped under nn.Parameter\n","    self.w_tensor = torch.nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n","    self.b_tensor = torch.nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n","  \n","  def forward(self,x):\n","    #Output prediction calculation\n","    return  w_tensor*x +b_tensor "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGLTnSTWo3r7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"outputId":"7ae5700a-08e9-458c-ab46-4182fbafe2f2","executionInfo":{"status":"ok","timestamp":1577958378494,"user_tz":-330,"elapsed":1353,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}}},"source":["#Model with PyTorch in-built methods\n","model = Network()\n","optimizer = torch.optim.SGD([w_tensor, b_tensor], lr=learning_rate) \n","loss = torch.nn.MSELoss(reduction='mean')\n","for i in range(epochs):\n","    print(\"-----------epoch:{}--------\".format(i))\n","    #This will not do actual training but will set model in training mode.\n","    model.train()\n","    \n","    #prediction\n","    y_pred = model(X_tensor) \n","\n","    #Error/loss calculation is Mean Squared Error\n","    error = loss(Y_tensor, y_pred)  \n","    print('Total Error:{}'.format(error))\n","\n","    '''Now no need to calculate gradients,PyTorch will do it if we tell which function/variable needs gradient calculation using backward()'''\n","    error.backward()\n","\n","    #Update weights using Optimizer\n","    optimizer.step()\n","\n","    #After each step,Reinitilaise gradients because PyTorch holds on to gradients\n","    #Reinitilaise gradients using Optimizer\n","    optimizer.zero_grad()   "],"execution_count":20,"outputs":[{"output_type":"stream","text":["-----------epoch:0--------\n","Total Error:292.89410400390625\n","-----------epoch:1--------\n","Total Error:292.6199645996094\n","-----------epoch:2--------\n","Total Error:292.3460388183594\n","-----------epoch:3--------\n","Total Error:292.0723571777344\n","-----------epoch:4--------\n","Total Error:291.7989196777344\n","-----------epoch:5--------\n","Total Error:291.5257263183594\n","-----------epoch:6--------\n","Total Error:291.2528381347656\n","-----------epoch:7--------\n","Total Error:290.98016357421875\n","-----------epoch:8--------\n","Total Error:290.7077941894531\n","-----------epoch:9--------\n","Total Error:290.43560791015625\n","-----------epoch:10--------\n","Total Error:290.1637268066406\n","-----------epoch:11--------\n","Total Error:289.8921203613281\n","-----------epoch:12--------\n","Total Error:289.6207580566406\n","-----------epoch:13--------\n","Total Error:289.3496398925781\n","-----------epoch:14--------\n","Total Error:289.0787658691406\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_6xgfVqVpgT8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"bc606269-3f82-4fa5-b20f-7be70a89ed42","executionInfo":{"status":"ok","timestamp":1577958810872,"user_tz":-330,"elapsed":1257,"user":{"displayName":"Sarang Mete","photoUrl":"","userId":"09604536810284816125"}}},"source":["'''To summarize,following are steps for model creation PyTorch:\n","1.Create Model class in which __init__() method contains trainable parameter and forward method contain rediction calculation\n","2.Intialise Optimizer and Loss function\n","3. Training Loop:\n","    model.train()--- Set model in training mode\n","    pred = model(X)-- Prediction\n","    loss = LossFunction(pred,actual)-- Loss calculation\n","    loss.backward()-- Gradient calculation\n","    optimizer.step()-- Update weights/parameters\n","    optimizer.zero_grad()-- Reset gradients'''"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'To summarize,following are steps for model creation PyTorch:\\n1.Create Model class in which __init__() method contains trainable parameter and forward method contain rediction calculation\\n2.Intialise Optimizer and Loss function\\n3. Training Loop:\\n    model.train()--- Set model in training mode\\n    pred = model(X)-- Prediction\\n    loss = LossFunction(pred,actual)-- Loss calculation\\n    loss.backward()-- Gradient calculation\\n    optimizer.step()-- Update weights/parameters\\n    optimizer.zero_grad()-- Reset gradients'"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"O5zzt8CgrJuT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}