{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyQnQ9gTCdUE",
        "colab_type": "text"
      },
      "source": [
        "Regression from Scratch in Numpy vs. PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMCnNSexCly1",
        "colab_type": "text"
      },
      "source": [
        "Regression in Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ_JDYDgOg0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cBkp4CiCskO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array([1,2,4,6,8,10,12,13,14,16,16,18,20,22,24])\n",
        "Y = np.array([39,42,43,46,47,56,60,59,64,66,68,72,71,75,80])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD1fuz_bCsZ0",
        "colab_type": "code",
        "outputId": "f8e65400-dc9c-409e-ca09-942ed2aaefd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.scatter(X,Y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f9637f97b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuiQVxFCXpIL8H7wM9jc4Qe2VAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIn7YAkCOzQv",
        "colab_type": "text"
      },
      "source": [
        "As we can see there is linear relationship between X and Y.(We'll discuss more about correlation in another post). We'll use linear regression to build prediction model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUgXRvdCsR4",
        "colab_type": "code",
        "outputId": "48cc89a3-3ddd-4f1f-be6d-a0df93cda5bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''Y = a*X+b is the equation of line/linear regression model.\n",
        "Goal here is to find the values of a and b.\n",
        "There are multiple techniques to achieve this:\n",
        "1.Matrix calculations: Put all data into matrices to perform optimization.Used for small dataset because of memory requirement.\n",
        "2.Gradient Descent : Try to minimize error/difference between actual and predicted values using derivatives.\n",
        "3.Regularization: While minimizing error,also try to reduce impact of unnecessary features.\n",
        "4.Simple linear regression:If there are single input varaible and single output variable,use covariance and variance to find a and b.\n",
        "\n",
        "More detailed explaination of above techniques is not in the scope here.\n",
        "We'll implement method 2 i.e Gradient Descent here-more specific-Batch Gradient Descent.\n",
        "Weights(a,b) are updated at end of complete batch/all rows as follow:\n",
        "new a = old a - (learning_rate*gradient_a)\n",
        "new b = old b - (learning_rate*gradient_b)\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Y = a*X+b is the equation of line/linear regression model.\\nGoal here is to find the values of a and b.\\nThere are multiple techniques to achieve this:\\n1.Matrix calculations: Put all data into matrices to perform optimization.Used for small dataset because of memory requirement.\\n2.Gradient Descent : Try to minimize error/difference between actual and predicted values using derivatives.\\n3.Regularization: While minimizing error,also try to reduce impact of unnecessary features.\\n4.Simple linear regression:If there are single input varaible and single output variable,use covariance and variance to find a and b.\\n\\nMore detailed explaination of above techniques is not in the scope here.\\nWe'll implement method 2 i.e Gradient Descent here-more specific-Batch Gradient Descent.\\nWeights(a,b) are updated at end of complete batch/all rows as follow:\\nnew a = old a - (learning_rate*gradient_a)\\nnew b = old b - (learning_rate*gradient_b)\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECaYuxM4zsIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "epochs=15\n",
        "learning_rate = 0.001\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "y_pred = np.empty(len(Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3449vzB3zsEG",
        "colab_type": "code",
        "outputId": "81bd35af-ac47-4174-942f-e08e65c28a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "for i in range(epochs):\n",
        "    print(\"-----------epoch:{}--------\".format(i))\n",
        "    #prediction\n",
        "    y_pred = w*X +b \n",
        "\n",
        "    #Error/loss calculation is Mean Squared Error\n",
        "    error = np.mean((Y - y_pred)**2)  \n",
        "    print('Total Error:{}'.format(error))\n",
        "\n",
        "    #Gradient calculation\n",
        "    gradient_a = np.mean(-2*X*(Y-y_pred))\n",
        "    gradient_b = np.mean(-2*(Y-y_pred))\n",
        "\n",
        "    #Update weights\n",
        "    w -= learning_rate*gradient_a\n",
        "    b -= learning_rate*gradient_b\n",
        "\n",
        "  \n",
        "   "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------epoch:0--------\n",
            "Total Error:4393.679411741339\n",
            "-----------epoch:1--------\n",
            "Total Error:1761.2933993839647\n",
            "-----------epoch:2--------\n",
            "Total Error:829.147694180196\n",
            "-----------epoch:3--------\n",
            "Total Error:498.94421785640026\n",
            "-----------epoch:4--------\n",
            "Total Error:381.8486450156428\n",
            "-----------epoch:5--------\n",
            "Total Error:340.20059390876725\n",
            "-----------epoch:6--------\n",
            "Total Error:325.26367603059253\n",
            "-----------epoch:7--------\n",
            "Total Error:319.7835764526846\n",
            "-----------epoch:8--------\n",
            "Total Error:317.6516893015521\n",
            "-----------epoch:9--------\n",
            "Total Error:316.705363217721\n",
            "-----------epoch:10--------\n",
            "Total Error:316.1789469743335\n",
            "-----------epoch:11--------\n",
            "Total Error:315.80137420708223\n",
            "-----------epoch:12--------\n",
            "Total Error:315.47667816740153\n",
            "-----------epoch:13--------\n",
            "Total Error:315.17088323869484\n",
            "-----------epoch:14--------\n",
            "Total Error:314.8719607426561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW2fVpbLbuyu",
        "colab_type": "code",
        "outputId": "16697e20-91c3-4931-990c-e9a5e810cd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(w,b)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.042799282999869 0.4771951521774575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Oas24OAqQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8e3647dd-3b4c-4c17-a3fc-5dcb51b2c0df"
      },
      "source": [
        "'''Error is reducing with increment in epochs. Number of epochs and learning rate are hyperparameters to tune. \n",
        "Let's not play around with it and jumpt to PyTorch'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Error is reducing with increment in epochs. Number of epochs and learning rate are hyperparameters to tune. \\nLet's not play around with it and jumpt to PyTorch\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmoHHxvV_X9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oht1yH1MCn9b",
        "colab_type": "text"
      },
      "source": [
        "Regression in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPONm4D4Ft0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT7zh_6_AKq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialise data/features and target\n",
        "X_tensor = torch.from_numpy(X)\n",
        "Y_tensor = torch.from_numpy(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTEuzbtFA5Ei",
        "colab_type": "code",
        "outputId": "11c394d4-e8cd-40e2-e69c-4ff96d051f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Initialise weights\n",
        "'''Here unlike numpy we have to mention that these variables are trainable(need to calculate derivatives).\n",
        "This can be done using requires_grad:'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here unlike numpy we have to mention that these variables are trainable(need to calculate derivatives).\\nThis can be done using requires_grad:'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP9l7K-YICz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.random.seed = 2\n",
        "w_tensor = torch.randn(1,requires_grad=True,dtype=torch.float)\n",
        "b_tensor = torch.randn(1,requires_grad=True,dtype=torch.float)\n",
        "epochs=15\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQHzW9E8cVSA",
        "colab_type": "code",
        "outputId": "b603b038-6c92-4ad6-f0d1-b4263f7be57a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w_tensor"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.2575], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PBFdPCdUz9a",
        "colab_type": "code",
        "outputId": "6bcb6965-4a28-41a3-df2b-d128a1cef553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Model without PyTorch in-built methods\n",
        "for i in range(epochs):\n",
        "    print(\"-----------epoch:{}--------\".format(i))\n",
        "    #prediction\n",
        "    y_pred = w_tensor*X_tensor +b_tensor \n",
        "\n",
        "    #Error/loss calculation is Mean Squared Error\n",
        "    error = ((Y_tensor - y_pred)**2).mean()\n",
        "    print('Total Error:{}'.format(error))\n",
        "\n",
        "    '''Now no need to calculate gradients,PyTorch will do it if we tell which function/variable needs gradient calculation using backward()'''\n",
        "    error.backward()\n",
        "\n",
        "    '''Actual values of gradients can be seen using grad attribute'''\n",
        "    #print(w_tensor.grad,b_tensor.grad)\n",
        "    \n",
        "    '''We can not directly use gradients in normal calculation,so use no_grad() method to get variables out of scope of computation graph '''\n",
        "    \n",
        "    with torch.no_grad():\n",
        "       w_tensor-= learning_rate*w_tensor.grad\n",
        "       b_tensor-= learning_rate*b_tensor.grad\n",
        "      \n",
        "\n",
        "     #After each step,Reinitilaise gradients because PyTorch holds on to gradients\n",
        "    w_tensor.grad.zero_()\n",
        "    b_tensor.grad.zero_()\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------epoch:0--------\n",
            "Total Error:6396.83935546875\n",
            "-----------epoch:1--------\n",
            "Total Error:2495.30615234375\n",
            "-----------epoch:2--------\n",
            "Total Error:1113.8157958984375\n",
            "-----------epoch:3--------\n",
            "Total Error:624.5057983398438\n",
            "-----------epoch:4--------\n",
            "Total Error:451.05792236328125\n",
            "-----------epoch:5--------\n",
            "Total Error:389.4356994628906\n",
            "-----------epoch:6--------\n",
            "Total Error:367.40386962890625\n",
            "-----------epoch:7--------\n",
            "Total Error:359.3885498046875\n",
            "-----------epoch:8--------\n",
            "Total Error:356.33563232421875\n",
            "-----------epoch:9--------\n",
            "Total Error:355.03997802734375\n",
            "-----------epoch:10--------\n",
            "Total Error:354.3665466308594\n",
            "-----------epoch:11--------\n",
            "Total Error:353.9134521484375\n",
            "-----------epoch:12--------\n",
            "Total Error:353.53887939453125\n",
            "-----------epoch:13--------\n",
            "Total Error:353.1919860839844\n",
            "-----------epoch:14--------\n",
            "Total Error:352.8553771972656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8o0xyqjI6C_",
        "colab_type": "code",
        "outputId": "abbac67c-f1a0-4e93-ff92-fbdb9168980c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Model with PyTorch in-built methods\n",
        "optimizer = torch.optim.SGD([w_tensor, b_tensor], lr=learning_rate) \n",
        "loss = torch.nn.MSELoss(reduction='mean')\n",
        "for i in range(epochs):\n",
        "    print(\"-----------epoch:{}--------\".format(i))\n",
        "    #prediction\n",
        "    y_pred = w_tensor*X_tensor +b_tensor \n",
        "\n",
        "    #Error/loss calculation is Mean Squared Error\n",
        "    error = loss(Y_tensor, y_pred)  \n",
        "    print('Total Error:{}'.format(error))\n",
        "\n",
        "    '''Now no need to calculate gradients,PyTorch will do it if we tell which function/variable needs gradient calculation using backward()'''\n",
        "    error.backward()\n",
        "\n",
        "    #Update weights using Optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    #After each step,Reinitilaise gradients because PyTorch holds on to gradients\n",
        "    #Reinitilaise gradients using Optimizer\n",
        "    optimizer.zero_grad()   "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------epoch:0--------\n",
            "Total Error:352.5224914550781\n",
            "-----------epoch:1--------\n",
            "Total Error:352.1910095214844\n",
            "-----------epoch:2--------\n",
            "Total Error:351.8605041503906\n",
            "-----------epoch:3--------\n",
            "Total Error:351.5302734375\n",
            "-----------epoch:4--------\n",
            "Total Error:351.20050048828125\n",
            "-----------epoch:5--------\n",
            "Total Error:350.87109375\n",
            "-----------epoch:6--------\n",
            "Total Error:350.5419616699219\n",
            "-----------epoch:7--------\n",
            "Total Error:350.2131042480469\n",
            "-----------epoch:8--------\n",
            "Total Error:349.8846435546875\n",
            "-----------epoch:9--------\n",
            "Total Error:349.55645751953125\n",
            "-----------epoch:10--------\n",
            "Total Error:349.22845458984375\n",
            "-----------epoch:11--------\n",
            "Total Error:348.9008483886719\n",
            "-----------epoch:12--------\n",
            "Total Error:348.5736389160156\n",
            "-----------epoch:13--------\n",
            "Total Error:348.2466735839844\n",
            "-----------epoch:14--------\n",
            "Total Error:347.91998291015625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcsTXlm9ZhHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e9cee805-510e-4145-8f98-216554ee3f94"
      },
      "source": [
        "'''Till now,we've explored loss calculation and Optimizers.\n",
        "The only manual step remaining is prediction step which also can be done by Model class,but we'll explore more on Model class and neural network in PyTorch in next \n",
        "article. If you liked the post CLAP!CLAP!CLAP! and your response is most WELCOME! '''\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Till now,we've explored loss calculation and Optimizers.\\nThe only manual step remaining is prediction step which also can be done by Model class,but we'll explore more on Model class and neural network in PyTorch in next \\narticle. If you liked the post CLAP!CLAP!CLAP! and your response is most WELCOME! \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI4z_msts1aG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}